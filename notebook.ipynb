{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Covid-19 Vaccines - Sentiment Analysis & Time Series**\n",
    "Notebook for the second project for the Machine Learning Complements course (CAC).\n",
    "\n",
    "## **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The following libraries will be used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import contractions\n",
    "import nltk\n",
    "import plotly.express as px\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "warnings.simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observations\n",
    "\n",
    "The dataset contains a single file: `tweets.csv`.\n",
    "\n",
    "In this section we will take a look at the first few rows of each file to get a better understanding of the data, and do some initial data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.initial_obs(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "We can see that many attributes are not really relevant for the kind of work we will be doing. Therefore, we'll just selec tthe most relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[['id','user_location','date','text','hashtags','user_followers']]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_tweets['text'].head(5)\n",
    "df_tweets['orig_text'] = df_tweets['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Spaces within the text\n",
    "When removing spaces within the text, ensure seamless integration of words for enhanced readability and processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.trim_text)\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions Mapping\n",
    "Contractions mapping simplifies language by expanding contractions like \"can't\" to \"cannot\" for consistent analysis and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(contractions.fix)\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML\n",
    "Cleaning HTML tags from text data streamlines content for NLP tasks, preventing interference from markup elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis & Emotion Handling\n",
    "Emojis and emotion handling enrich text analysis by capturing nuances of sentiment and expression for deeper understanding. We thought about removing them initially, however their presence may be crucial to identify sentiments within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251\\U0001F004\\U0001F0CF\\U0001F170-\\U0001F251\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF]+', flags=re.UNICODE)\n",
    "\n",
    "# Find examples in df_tweets['text'] that have emojis\n",
    "emojis_examples = df_tweets[df_tweets['text'].str.contains(pattern, na=False)][0:5]\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    print(df_tweets.loc[index, 'text'])\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].apply(ut.convert_emojis_to_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    emoji_text = df_tweets.loc[index, 'text']\n",
    "    print(emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Twitter Handles (@) & Hashtags\n",
    "Handling Twitter handles (@) and hashtags facilitates contextual analysis and topic extraction in social media text. We removed the twitter handle, as they mostly are used to identify persons therefore they are not very important in this matter. On the other hand, hashtags may indicate sentiments or other important informations like topics. e.g #sad, #happy or #astrozeneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.remove_twitter_handles_hashtags)\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    emoji_text = df_tweets.loc[index, 'text']\n",
    "    print(emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lower-case\n",
    "Converting all the characters to lower case so that words in different forms can be interpreted as the same. The problem with this is that in social media people may use upper-case to express sentiments, e.g SAD, HAPPY.\n",
    "\n",
    "Here we also remove special characters, keeping only characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.remove_special_characters)\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization breaks down text into individual units, such as words or phrases, enabling granular analysis and feature extraction. We also remove stopwords, meaning words that often appear within the text and don't add any meaning to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tokenized_text'] = df_tweets['text'].apply(lambda x: word_tokenize(x))\n",
    "df_tweets['tokenized_text'] = df_tweets['tokenized_text'].apply(ut.remove_stopwords)\n",
    "df_tweets['token_text'] = df_tweets['tokenized_text'].apply(lambda text: \" \".join(text))\n",
    "\n",
    "\n",
    "df_tweets['tokenized_text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming typically chops off prefixes and/or suffixes of words to derive the root form. It's a simpler and faster process compared to lemmatization. However, stemming doesn't always result in valid words. For instance, \"running\" might be stemmed to \"runn,\" which isn't a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df_tweets['stemmed_text'] = df_tweets['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "df_tweets['stemmed_text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization, on the other hand, involves resolving words to their dictionary form, known as the lemma. It uses lexical knowledge bases to ensure that the root form returned is a valid word. For example, \"am,\" \"are,\" and \"is\" would all be lemmatized to \"be.\" Lemmatization is generally more accurate than stemming but can be slower due to its linguistic complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_tweets['lemmatized_text'] = df_tweets['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "df_tweets['lemmatized_text'].head(5)\n",
    "df_tweets['clean_text'] = df_tweets['lemmatized_text'].apply(lambda text: \" \".join(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Using VADER & TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    vader_scores = sid.polarity_scores(text)['compound']\n",
    "    if vader_scores >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif vader_scores <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    return sentiment, vader_scores\n",
    "\n",
    "df_tweets['sentiment'], df_tweets['vader_score'] = zip(*df_tweets['text'].apply(analyze_sentiment))\n",
    "#df_tweets['sentiment'] = df_tweets['sentiment'].replace({'Positive': 1, 'Neutral': 0, 'Negative': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_sentiments(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = df_tweets[df_tweets['sentiment'] == \"Positive\"]\n",
    "negative_tweets = df_tweets[df_tweets['sentiment'] == \"Negative\"]\n",
    "neutral_tweets = df_tweets[df_tweets['sentiment'] == \"Neutral\"]\n",
    "\n",
    "ut.generate_word_cloud(positive_tweets['token_text'], 'Positive Sentiment Word Cloud')\n",
    "positive_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(positive_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutral Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.generate_word_cloud(neutral_tweets['token_text'], 'Neutral Sentiment Word Cloud')\n",
    "neutral_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(neutral_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.generate_word_cloud(negative_tweets['token_text'], 'Negative Sentiment Word Cloud')\n",
    "negative_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(negative_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Analysis by sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uni-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Average Word Amount by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_avg_word_length_distribution_multi(positive_tweets, neutral_tweets, negative_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Typical ML Approach\n",
    "As we can see our dataset is not labelled, therefore we can't separate it into train/test and just train a model. What we will do is train a model using a labelled tweet dataset (the theme of both dataset would be similar, so we can use it for training) and then test on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training_df = pd.read_csv('train_tweets.csv')\n",
    "training_df = training_df[training_df['new_sentiment'].notna()]\n",
    "training_df = training_df[['old_text','new_sentiment']]\n",
    "\n",
    "\n",
    "training_df = ut.pre_process_pipeline(training_df,'old_text')\n",
    "training_df.rename(columns={'new_sentiment': 'sentiment'}, inplace=True)\n",
    "training_df['sentiment'] = training_df['sentiment'].replace({'positive': 1, 'neutral': 0, 'negative': -1})\n",
    "\n",
    "ut.initial_obs(training_df)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features, test_features = ut.generate_features(training_df,df_tweets,'tfidf')\n",
    "\n",
    "#TF-IDF Results:\n",
    "pred_nb_tfidf, pred_svm_tfidf = ut.predict_labels(train_features,training_df['sentiment'].values,test_features)\n",
    "\n",
    "df_tweets['predicted_tfidf_NB'] = pred_nb_tfidf\n",
    "df_tweets['predicted_tfidf_SVM'] = pred_svm_tfidf\n",
    "\n",
    "\n",
    "df_eval = df_tweets.copy()\n",
    "df_eval = df_eval[['clean_text','text','sentiment','predicted_tfidf_NB','predicted_tfidf_SVM']]\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sentiment_counts = df_eval['predicted_tfidf_NB'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"viridis\")\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\"\"\"\n",
    "vectorizer = CountVectorizer(max_features=1000, \n",
    "                             stop_words='english')\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(df_tweets['text'])\"\"\"\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_tweets['clean_text'])\n",
    "\n",
    "# Step 3: Apply LDA\n",
    "num_topics = 8  # You can adjust the number of topics\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Step 4: Display the topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx+1}:\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 15  # Number of top words to display for each topic\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Spatial Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the data to a single date and print tweets from users with the most followers\n",
    "def date_filter(df, date):\n",
    "    return df[df['date'].astype(str)==date].sort_values('user_followers', ascending=False)[['date' ,'orig_text']]\n",
    "\n",
    "def date_printer(df, dates, num=10): \n",
    "    for date in dates:\n",
    "        display(date_filter(df, date).head(num))\n",
    "        \n",
    "# Get tweets for vaccine Pfizer,Pfizer; Sinopharm;Sinovac;Moderna;AstraZeneca;Covaxin;Sputnik V.\n",
    "df_tweets_pz = df_tweets[df_tweets['clean_text'].str.contains('pfizer', case=False, na=False)]\n",
    "df_tweets_sinopharm = df_tweets[df_tweets['clean_text'].str.contains('sinopharm', case=False, na=False)]\n",
    "df_tweets_sinovac = df_tweets[df_tweets['clean_text'].str.contains('sinovac', case=False, na=False)]\n",
    "df_tweets_moderna = df_tweets[df_tweets['clean_text'].str.contains('moderna', case=False, na=False)]\n",
    "df_tweets_astrazeneca = df_tweets[df_tweets['clean_text'].str.contains('astrazeneca', case=False, na=False)]\n",
    "df_tweets_covaxin = df_tweets[df_tweets['clean_text'].str.contains('covaxin', case=False, na=False)]\n",
    "df_tweets_sputnik = df_tweets[df_tweets['clean_text'].str.contains('sputnik', case=False, na=False)]\n",
    "\n",
    "print('Number of tweets for Pfizer:', len(df_tweets_pz))\n",
    "print('Number of tweets for Sinopharm:', len(df_tweets_sinopharm))\n",
    "print('Number of tweets for Sinovac:', len(df_tweets_sinovac))\n",
    "print('Number of tweets for Moderna:', len(df_tweets_moderna))\n",
    "print('Number of tweets for AstraZeneca:', len(df_tweets_astrazeneca))\n",
    "print('Number of tweets for Covaxin:', len(df_tweets_covaxin))\n",
    "print('Number of tweets for Sputnik V:', len(df_tweets_sputnik))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['date'] = pd.to_datetime(df_tweets['date'])\n",
    "df_tweets['date'] = df_tweets['date'].dt.date\n",
    "\n",
    "# Get counts of number of tweets by sentiment for each date\n",
    "timeline = df_tweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\n",
    "\n",
    "# Plot results\n",
    "\n",
    "fig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},color_discrete_sequence=[ '#EF553B','#636EFA', '#00CC96'], title='Number of Tweets by Sentiment Over Time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some spikes in the data, which may be due to some events that happened in the world. Let's investigate them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike = df_tweets[df_tweets['date'].astype(str)=='2021-03-01']\n",
    "spike['user_location'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike = spike.sort_values('user_location', ascending=False)\n",
    "spike['orig_text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the Prime Minister of India took the first dose of the Covid 19 vaccine on March 1st, 2021. This event caused a spike in the number of tweets and we can see that the sentiment is mostly positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covaxin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_timeline(df, vax, title):\n",
    "    df = df.dropna()\n",
    "    title_str = 'Timeline showing sentiment of tweets about the '+title+' vaccine'\n",
    "    vac_tweets = df_tweets[df_tweets['clean_text'].str.contains(vax, case=False, na=False)]\n",
    "    \n",
    "    timeline = vac_tweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n",
    "    fig = px.line(timeline, x='date', y='tweets', color='sentiment', color_discrete_map={'Positive': '#00CC96', 'Negative': '#EF553B', 'Neutral': '#636EFA'},title=title_str)\n",
    "\n",
    "    fig.show()\n",
    "    return vac_tweets\n",
    "\n",
    "covaxin = filtered_timeline(df_tweets, 'covaxin', title='Covaxin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxin_spike = covaxin[covaxin['date'].astype(str)=='2021-11-03']\n",
    "print('Number of tweets on 2021-11-03:', len(covaxin_spike))\n",
    "covaxin_spike['user_location'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxin_spike = covaxin[covaxin['user_location']=='India']\n",
    "\n",
    "date_printer(covaxin_spike, ['2021-11-03'], num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a brief investigation we found out that Covaxin was approved for emergency use by WHO (World Health Organization) on May 3rd, 2021. This led to a positive spike in the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sinopharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinopharm = filtered_timeline(df_tweets, 'sinopharm', title='Sinopharm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinopharm_spike = sinopharm[sinopharm['date'].astype(str)=='2021-08-13']\n",
    "print('Number of tweets on July 13th:', len(sinopharm_spike))\n",
    "\n",
    "\n",
    "print(sinopharm_spike['orig_text'].head(10))\n",
    "\n",
    "# Count how many duplicate tweets there are\n",
    "print('Number of duplicate tweets:', len(sinopharm_spike[sinopharm_spike.duplicated(subset='clean_text')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Sinopharm vaccine there was one major event that happened. An 89 year old man died after taking the vaccine. This event lead to a huge number of negative tweets.\n",
    "Additionally, we observed a notable prevalence of duplicate data within the dataset, with 821 similar tweets recorded on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sinovac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinovac = filtered_timeline(df_tweets, 'sinovac', title='Sinovac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinovac_spike = sinovac[sinovac['date'].astype(str)=='2021-06-02']\n",
    "\n",
    "print('Number of tweets on June 2nd:', len(sinovac_spike))\n",
    "\n",
    "date_printer(sinovac_spike, ['2021-06-02'], num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Covaxin vaccine, the Sinovac vaccine was also approved for emergency use by WHO on June 6th, 2021. This event led to a mostly positive sentiment in the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moderna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderna = filtered_timeline(df_tweets, 'moderna', title='Moderna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were numerous spikes related to the Moderna vaccine. Let's focus on the positive spike that happened on June 29th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_printer(moderna, ['2021-06-29'], num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On June 29th, 2021, India granted approval for the importation and utilization of the Moderna vaccine. This development sparked a notable increase in positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AztraZeneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astrazeneca = filtered_timeline(df_tweets, 'astrazeneca', title='AstraZeneca')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sputnik V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sputnik = filtered_timeline(df_tweets, 'sputnik', title='Sputnik V')\n",
    "\n",
    "\n",
    "dates = ['2021-04-12', '2021-05-14']\n",
    "\n",
    "date_printer(sputnik, dates, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pfizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer = filtered_timeline(df_tweets, 'pfizer', title='Pfizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORK IN PROGRESS\n",
    "locations = {\n",
    "    'India': {'lat': 23.309469, 'long': 78.532748},\n",
    "    'United States': {'lat': 36.434542, 'long': -103.931671},\n",
    "    'China': {'lat': 35.377854, 'long': 103.165949},\n",
    "}\n",
    "country_counts = {country: 0 for country in locations.keys()}\n",
    "for index, tweet in df_tweets.iterrows():\n",
    "    user_location = str(tweet['user_location'])  # Convert to string\n",
    "    # If the user location contains a country name\n",
    "    for country in locations.keys():\n",
    "        if country in user_location:\n",
    "            country_counts[country] += 1  # Increment count for the country\n",
    "            break\n",
    "\n",
    "data = []\n",
    "for country, count in country_counts.items():\n",
    "    data.append({'Location': country, 'Latitude': locations[country]['lat'], 'Longitude': locations[country]['long'], 'Number of Tweets': count})\n",
    "\n",
    "df_locations = pd.DataFrame(data)\n",
    "\n",
    "print(df_locations)\n",
    "fig = px.scatter_mapbox(df_locations, lat=\"Latitude\", lon=\"Longitude\", hover_name=\"Number of Tweets\", size = 'Number of Tweets',color=\"Number of Tweets\", \n",
    "                    color_continuous_scale=px.colors.sequential.Plasma, size_max=15, zoom=1,\n",
    "                   mapbox_style=\"carto-positron\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
