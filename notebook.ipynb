{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Covid-19 Vaccines - Sentiment Analysis & Time Series**\n",
    "Notebook for the second project for the Machine Learning Complements course (CAC).\n",
    "\n",
    "## **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The following libraries will be used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import contractions\n",
    "import nltk\n",
    "import plotly.express as px\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "warnings.simplefilter(action='ignore')\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observations\n",
    "\n",
    "The dataset contains a single file: `tweets.csv`.\n",
    "\n",
    "In this section we will take a look at the first few rows of each file to get a better understanding of the data, and do some initial data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.initial_obs(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "We can see that many attributes are not really relevant for the kind of work we will be doing. Therefore, we'll just selec tthe most relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[['id','user_location','date','text','hashtags','user_followers','source']]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_tweets['text'].head(5)\n",
    "df_tweets['orig_text'] = df_tweets['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Spaces within the text\n",
    "When removing spaces within the text, ensure seamless integration of words for enhanced readability and processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.trim_text)\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions Mapping\n",
    "Contractions mapping simplifies language by expanding contractions like \"can't\" to \"cannot\" for consistent analysis and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(contractions.fix)\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML\n",
    "Cleaning HTML tags from text data streamlines content for NLP tasks, preventing interference from markup elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis & Emotion Handling\n",
    "Emojis and emotion handling enrich text analysis by capturing nuances of sentiment and expression for deeper understanding. We thought about removing them initially, however their presence may be crucial to identify sentiments within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251\\U0001F004\\U0001F0CF\\U0001F170-\\U0001F251\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF]+', flags=re.UNICODE)\n",
    "\n",
    "# Find examples in df_tweets['text'] that have emojis\n",
    "emojis_examples = df_tweets[df_tweets['text'].str.contains(pattern, na=False)][0:5]\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    print(df_tweets.loc[index, 'text'])\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].apply(ut.convert_emojis_to_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    emoji_text = df_tweets.loc[index, 'text']\n",
    "    print(emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Twitter Handles (@) & Hashtags\n",
    "Handling Twitter handles (@) and hashtags facilitates contextual analysis and topic extraction in social media text. We removed the twitter handle, as they mostly are used to identify persons therefore they are not very important in this matter. On the other hand, hashtags may indicate sentiments or other important informations like topics. e.g #sad, #happy or #astrozeneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.remove_twitter_handles_hashtags)\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    emoji_text = df_tweets.loc[index, 'text']\n",
    "    print(emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lower-case\n",
    "Converting all the characters to lower case so that words in different forms can be interpreted as the same. The problem with this is that in social media people may use upper-case to express sentiments, e.g SAD, HAPPY.\n",
    "\n",
    "Here we also remove special characters, keeping only characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.remove_special_characters)\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization breaks down text into individual units, such as words or phrases, enabling granular analysis and feature extraction. We also remove stopwords, meaning words that often appear within the text and don't add any meaning to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tokenized_text'] = df_tweets['text'].apply(lambda x: word_tokenize(x))\n",
    "df_tweets['tokenized_text'] = df_tweets['tokenized_text'].apply(ut.remove_stopwords)\n",
    "df_tweets['token_text'] = df_tweets['tokenized_text'].apply(lambda text: \" \".join(text))\n",
    "\n",
    "\n",
    "df_tweets['tokenized_text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming typically chops off prefixes and/or suffixes of words to derive the root form. It's a simpler and faster process compared to lemmatization. However, stemming doesn't always result in valid words. For instance, \"running\" might be stemmed to \"runn,\" which isn't a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df_tweets['stemmed_text'] = df_tweets['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "df_tweets['stemmed_text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization, on the other hand, involves resolving words to their dictionary form, known as the lemma. It uses lexical knowledge bases to ensure that the root form returned is a valid word. For example, \"am,\" \"are,\" and \"is\" would all be lemmatized to \"be.\" Lemmatization is generally more accurate than stemming but can be slower due to its linguistic complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_tweets['lemmatized_text'] = df_tweets['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "df_tweets['lemmatized_text'].head(5)\n",
    "df_tweets['clean_text'] = df_tweets['lemmatized_text'].apply(lambda text: \" \".join(text))\n",
    "df_tweets.drop_duplicates(subset='clean_text', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Using VADER & TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    vader_scores = sid.polarity_scores(text)['compound']\n",
    "    if vader_scores >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif vader_scores <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    return sentiment, vader_scores\n",
    "\n",
    "df_tweets['sentiment'], df_tweets['vader_score'] = zip(*df_tweets['clean_text'].apply(analyze_sentiment))\n",
    "\n",
    "#df_tweets['sentiment'] = df_tweets['sentiment'].replace({'Positive': 1, 'Neutral': 0, 'Negative': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_sentiments(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = df_tweets[df_tweets['sentiment'] == \"Positive\"]\n",
    "negative_tweets = df_tweets[df_tweets['sentiment'] == \"Negative\"]\n",
    "neutral_tweets = df_tweets[df_tweets['sentiment'] == \"Neutral\"]\n",
    "\n",
    "ut.generate_word_cloud(positive_tweets['token_text'], 'Positive Sentiment Word Cloud')\n",
    "positive_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(positive_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutral Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.generate_word_cloud(neutral_tweets['token_text'], 'Neutral Sentiment Word Cloud')\n",
    "neutral_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(neutral_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.generate_word_cloud(negative_tweets['token_text'], 'Negative Sentiment Word Cloud')\n",
    "negative_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(negative_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Analysis by sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uni-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Average Word Amount by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_avg_word_length_distribution_multi(positive_tweets, neutral_tweets, negative_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Spatial Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter the data to a single date and print tweets from users with the most followers\n",
    "def date_filter(df, date):\n",
    "    return df[df['date'].astype(str)==date].sort_values('user_followers', ascending=False)[['date' ,'orig_text']]\n",
    "\n",
    "def date_printer(df, dates, num=10): \n",
    "    for date in dates:\n",
    "        display(date_filter(df, date).head(num))\n",
    "        \n",
    "# Get tweets for vaccine Pfizer,Pfizer; Sinopharm;Sinovac;Moderna;AstraZeneca;Covaxin;Sputnik V.\n",
    "df_tweets_pz = df_tweets[df_tweets['clean_text'].str.contains('pfizer', case=False, na=False)]\n",
    "df_tweets_sinopharm = df_tweets[df_tweets['clean_text'].str.contains('sinopharm', case=False, na=False)]\n",
    "df_tweets_sinovac = df_tweets[df_tweets['clean_text'].str.contains('sinovac', case=False, na=False)]\n",
    "df_tweets_moderna = df_tweets[df_tweets['clean_text'].str.contains('moderna', case=False, na=False)]\n",
    "df_tweets_astrazeneca = df_tweets[df_tweets['clean_text'].str.contains('astrazeneca', case=False, na=False)]\n",
    "df_tweets_covaxin = df_tweets[df_tweets['clean_text'].str.contains('covaxin', case=False, na=False)]\n",
    "df_tweets_sputnik = df_tweets[df_tweets['clean_text'].str.contains('sputnik', case=False, na=False)]\n",
    "\n",
    "print('Number of tweets for Pfizer:', len(df_tweets_pz))\n",
    "print('Number of tweets for Sinopharm:', len(df_tweets_sinopharm))\n",
    "print('Number of tweets for Sinovac:', len(df_tweets_sinovac))\n",
    "print('Number of tweets for Moderna:', len(df_tweets_moderna))\n",
    "print('Number of tweets for AstraZeneca:', len(df_tweets_astrazeneca))\n",
    "print('Number of tweets for Covaxin:', len(df_tweets_covaxin))\n",
    "print('Number of tweets for Sputnik V:', len(df_tweets_sputnik))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Assuming df_tweets is already defined and contains a 'date' column\n",
    "df_tweets['date_'] = pd.to_datetime(df_tweets['date']).dt.date\n",
    "\n",
    "# Count tweets per day\n",
    "tweets_per_day = df_tweets.groupby('date_').size().reset_index(name='Tweets Per Day')\n",
    "\n",
    "# Create the plot using Plotly Express\n",
    "fig = px.line(tweets_per_day, x='date_', y='Tweets Per Day')\n",
    "\n",
    "# Add a horizontal line representing the mean of tweet counts\n",
    "mean_tweet_count = tweets_per_day['Tweets Per Day'].mean()\n",
    "fig.add_shape(type=\"line\",\n",
    "    x0=tweets_per_day['date_'].min(), y0=mean_tweet_count, x1=tweets_per_day['date_'].max(), y1=mean_tweet_count,\n",
    "    line=dict(\n",
    "        color=\"Green\",\n",
    "        width=3,\n",
    "        dash=\"dashdot\",\n",
    "    ),\n",
    "    name='Mean',\n",
    ")\n",
    "\n",
    "# Update the traces to include markers\n",
    "fig.update_traces(mode=\"markers+lines\")\n",
    "\n",
    "# Add annotations\n",
    "annotations = [\n",
    "    dict(\n",
    "        x=datetime.datetime(2021, 3, 1), \n",
    "        y=tweets_per_day.loc[tweets_per_day['date_'] == datetime.date(2021, 3, 1), 'Tweets Per Day'].values[0],\n",
    "        text='March 1',\n",
    "        showarrow=True,\n",
    "        arrowhead=3,\n",
    "        bordercolor=\"#c7c7c7\"\n",
    "    ),\n",
    "    dict(\n",
    "        x=datetime.datetime(2021, 4, 21), \n",
    "        y=tweets_per_day.loc[tweets_per_day['date_'] == datetime.date(2021, 4, 21), 'Tweets Per Day'].values[0],\n",
    "        text='April 21',\n",
    "        showarrow=True,\n",
    "        arrowhead=3,\n",
    "        yshift=5,\n",
    "        bordercolor=\"#c7c7c7\"\n",
    "    ),\n",
    "    dict(\n",
    "        x=datetime.datetime(2021, 6, 30), \n",
    "        y=tweets_per_day.loc[tweets_per_day['date_'] == datetime.date(2021, 6, 30), 'Tweets Per Day'].values[0],\n",
    "        text='June 30',\n",
    "        showarrow=True,\n",
    "        arrowhead=3,\n",
    "        yshift=5,\n",
    "        ay=-30,\n",
    "        bordercolor=\"#c7c7c7\"\n",
    "    ),\n",
    "    dict(\n",
    "        x=datetime.datetime(2021, 8, 11), \n",
    "        y=tweets_per_day.loc[tweets_per_day['date_'] == datetime.date(2021, 8, 11), 'Tweets Per Day'].values[0],\n",
    "        text='August 11',\n",
    "        showarrow=True,\n",
    "        arrowhead=3,\n",
    "        yshift=5,\n",
    "        ay=-30,\n",
    "        bordercolor=\"#c7c7c7\"\n",
    "    ),dict(\n",
    "        x=datetime.datetime(2021, 10, 12), \n",
    "        y=tweets_per_day.loc[tweets_per_day['date_'] == datetime.date(2021, 10, 12), 'Tweets Per Day'].values[0],\n",
    "        text='October 12',\n",
    "        showarrow=True,\n",
    "        arrowhead=3,\n",
    "        yshift=5,\n",
    "        ay=-30,\n",
    "        bordercolor=\"#c7c7c7\"\n",
    "    ),dict(\n",
    "        x=datetime.datetime(2021, 11, 3), \n",
    "        y=tweets_per_day.loc[tweets_per_day['date_'] == datetime.date(2021, 11, 3), 'Tweets Per Day'].values[0],\n",
    "        text='November 3',\n",
    "        showarrow=True,\n",
    "        arrowhead=3,\n",
    "        yshift=5,\n",
    "        ay=-30,\n",
    "        bordercolor=\"#c7c7c7\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for annotation in annotations:\n",
    "    fig.add_annotation(annotation)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='<b>Daily Tweets<b>',\n",
    "    hovermode='x unified',\n",
    "    width=1000\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "dates_to_check = ['2021-03-01','2021-04-21','2021-06-30','2021-08-11','2021-10-12','2021-11-03']\n",
    "\n",
    "for date in dates_to_check:\n",
    "    print(\"DATE: \", date)\n",
    "    tweets_data = ut.get_tweets_by_date(df_tweets, date)\n",
    "    ut.topic_modelling(tweets_data)\n",
    "    print('---------------------------\\n\\n')\n",
    "\n",
    "#for i in range(1,100):\n",
    "    #print(\"index: \", i, tweets_on_specific_date.iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_tweets[df_tweets['sentiment'] != 'Neutral']\n",
    "\n",
    "# Group by 'user_location', calculate the average sentiment, and select the top 10 locations\n",
    "top_10_locations = df_filtered.groupby('user_location')['sentiment'].value_counts().unstack().fillna(0).sum(axis=1).nlargest(10).index\n",
    "\n",
    "# Filter DataFrame to include only the top 10 locations\n",
    "df_top_10 = df_filtered[df_filtered['user_location'].isin(top_10_locations)]\n",
    "\n",
    "# Group by 'user_location' and calculate the average sentiment\n",
    "avg_sentiment = df_top_10.groupby('user_location')['sentiment'].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=avg_sentiment.index, y=avg_sentiment['Positive'], color='green', label='Positive')\n",
    "sns.barplot(x=avg_sentiment.index, y=avg_sentiment['Negative'], color='red', label='Negative')\n",
    "plt.ylabel('Average Sentiment')\n",
    "plt.xlabel('User Location')\n",
    "plt.title('Average Sentiment per Top 10 User Locations')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_data = ut.get_tweets_by_date(df_tweets, '2021-11-03')\n",
    "#for i in range(1,2000):\n",
    "#   print(\"index: \", i, tweets_data.iloc[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['date'] = pd.to_datetime(df_tweets['date'])\n",
    "df_tweets['datedt'] = pd.to_datetime(df_tweets['date'])\n",
    "df_tweets['date'] = df_tweets['date'].dt.date\n",
    "df_tweets['year'] = df_tweets['datedt'].dt.year\n",
    "df_tweets['month'] = df_tweets['datedt'].dt.month\n",
    "df_tweets['day'] = df_tweets['datedt'].dt.day\n",
    "df_tweets['dayofweek'] = df_tweets['datedt'].dt.dayofweek\n",
    "df_tweets['hour'] = df_tweets['datedt'].dt.hour\n",
    "df_tweets['minute'] = df_tweets['datedt'].dt.minute\n",
    "df_tweets['dayofyear'] = df_tweets['datedt'].dt.dayofyear\n",
    "df_tweets['date_only'] = df_tweets['datedt'].dt.date\n",
    "\n",
    "\n",
    "\n",
    "# Get counts of number of tweets by sentiment for each date\n",
    "timeline = df_tweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index().dropna()\n",
    "\n",
    "# Plot results\n",
    "\n",
    "fig = px.line(timeline, x='date', y='tweets', color='sentiment', category_orders={'sentiment': ['neutral', 'negative', 'positive']},color_discrete_sequence=[ '#EF553B','#636EFA', '#00CC96'], title='Number of Tweets by Sentiment Over Time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some spikes in the data, which may be due to some events that happened in the world. Let's investigate them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike = df_tweets[df_tweets['date'].astype(str)=='2021-03-01']\n",
    "spike['user_location'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike = spike.sort_values('user_location', ascending=False)\n",
    "spike['orig_text'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the Prime Minister of India took the first dose of the Covid 19 vaccine on March 1st, 2021. This event caused a spike in the number of tweets and we can see that the sentiment is mostly positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Covaxin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_timeline(df, vax, title):\n",
    "    df = df.dropna()\n",
    "    title_str = 'Timeline showing sentiment of tweets about the '+title+' vaccine'\n",
    "    vac_tweets = df_tweets[df_tweets['clean_text'].str.contains(vax, case=False, na=False)]\n",
    "    \n",
    "    timeline = vac_tweets.groupby(['date', 'sentiment']).agg(**{'tweets': ('id', 'count')}).reset_index()\n",
    "    fig = px.line(timeline, x='date', y='tweets', color='sentiment', color_discrete_map={'Positive': '#00CC96', 'Negative': '#EF553B', 'Neutral': '#636EFA'},title=title_str)\n",
    "\n",
    "    fig.show()\n",
    "    return vac_tweets\n",
    "\n",
    "covaxin = filtered_timeline(df_tweets, 'covaxin', title='Covaxin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxin_spike = covaxin[covaxin['date'].astype(str)=='2021-11-03']\n",
    "print('Number of tweets on 2021-11-03:', len(covaxin_spike))\n",
    "covaxin_spike['user_location'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxin_spike = covaxin[covaxin['user_location']=='India']\n",
    "\n",
    "date_printer(covaxin_spike, ['2021-11-03'], num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a brief investigation we found out that Covaxin was approved for emergency use by WHO (World Health Organization) on May 3rd, 2021. This led to a positive spike in the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sinopharm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinopharm = filtered_timeline(df_tweets, 'sinopharm', title='Sinopharm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinopharm_spike = sinopharm[sinopharm['date'].astype(str)=='2021-08-13']\n",
    "print('Number of tweets on July 13th:', len(sinopharm_spike))\n",
    "\n",
    "\n",
    "print(sinopharm_spike['orig_text'].head(10))\n",
    "\n",
    "# Count how many duplicate tweets there are\n",
    "print('Number of duplicate tweets:', len(sinopharm_spike[sinopharm_spike.duplicated(subset='clean_text')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Sinopharm vaccine there was one major event that happened. An 89 year old man died after taking the vaccine. This event lead to a huge number of negative tweets.\n",
    "Additionally, we observed a notable prevalence of duplicate data within the dataset, with 821 similar tweets recorded on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sinovac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinovac = filtered_timeline(df_tweets, 'sinovac', title='Sinovac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinovac_spike = sinovac[sinovac['date'].astype(str)=='2021-06-02']\n",
    "\n",
    "print('Number of tweets on June 2nd:', len(sinovac_spike))\n",
    "\n",
    "date_printer(sinovac_spike, ['2021-06-02'], num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Covaxin vaccine, the Sinovac vaccine was also approved for emergency use by WHO on June 6th, 2021. This event led to a mostly positive sentiment in the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moderna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderna = filtered_timeline(df_tweets, 'moderna', title='Moderna')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were numerous spikes related to the Moderna vaccine. Let's focus on the positive spike that happened on June 29th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_printer(moderna, ['2021-06-29'], num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On June 29th, 2021, India granted approval for the importation and utilization of the Moderna vaccine. This development sparked a notable increase in positive tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AztraZeneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astrazeneca = filtered_timeline(df_tweets, 'astrazeneca', title='AstraZeneca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_printer(astrazeneca, ['2021-03-16'], num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sputnik V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sputnik = filtered_timeline(df_tweets, 'sputnik', title='Sputnik V')\n",
    "\n",
    "\n",
    "dates = ['2021-04-12', '2021-05-14']\n",
    "\n",
    "date_printer(sputnik, dates, num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pfizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfizer = filtered_timeline(df_tweets, 'pfizer', title='Pfizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_printer(pfizer, ['2021-08-23'], num=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest peak of positive engagement for the Pfizer vaccine seems to relate to it's approval by the US FDA, the first vaccine to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORK IN PROGRESS\n",
    "locations = {\n",
    "    'India': {'lat': 23.309469, 'long': 78.532748},\n",
    "    'United States': {'lat': 36.434542, 'long': -103.931671},\n",
    "    'China': {'lat': 35.377854, 'long': 103.165949},\n",
    "}\n",
    "country_counts = {country: 0 for country in locations.keys()}\n",
    "for index, tweet in df_tweets.iterrows():\n",
    "    user_location = str(tweet['user_location'])  # Convert to string\n",
    "    # If the user location contains a country name\n",
    "    for country in locations.keys():\n",
    "        if country in user_location:\n",
    "            country_counts[country] += 1  # Increment count for the country\n",
    "            break\n",
    "\n",
    "data = []\n",
    "for country, count in country_counts.items():\n",
    "    data.append({'Location': country, 'Latitude': locations[country]['lat'], 'Longitude': locations[country]['long'], 'Number of Tweets': count})\n",
    "\n",
    "df_locations = pd.DataFrame(data)\n",
    "\n",
    "print(df_locations)\n",
    "fig = px.scatter_mapbox(df_locations, lat=\"Latitude\", lon=\"Longitude\", hover_name=\"Number of Tweets\", size = 'Number of Tweets',color=\"Number of Tweets\", \n",
    "                    color_continuous_scale=px.colors.sequential.Plasma, size_max=15, zoom=1,\n",
    "                   mapbox_style=\"carto-positron\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(df, title, x, y, color, color_discrete_sequence, angle, size, rge=0, scale=''):\n",
    "    # Plot the mean sentiment scores for each vaccine\n",
    "\n",
    "    if(scale!=''):\n",
    "        fig = px.bar(df, x, y, title=title, labels={x: x, y: y}, color=color, color_continuous_scale=scale)\n",
    "    else:\n",
    "        if(rge==0):\n",
    "            fig = px.bar(df, x, y, title=title, labels={x: x, y: y}, color=color, color_discrete_sequence=color_discrete_sequence)\n",
    "        else:\n",
    "            fig = px.bar(df, x, y, title=title, labels={x: x, y: y}, color=color, color_discrete_sequence=color_discrete_sequence, range_y=[-rge, rge])\n",
    "\n",
    "    # Rotate x-axis labels for better readability\n",
    "    fig.update_layout(xaxis_tickangle=angle)\n",
    "    # bigger figure\n",
    "    fig.update_layout(width=size[0], height=size[1])\n",
    "        \n",
    "    # Show plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each location\n",
    "location_counts = df_tweets['user_location'].value_counts().reset_index()\n",
    "location_counts.columns = ['user_location', 'count']\n",
    "\n",
    "# Sort locations by count in descending order\n",
    "location_counts = location_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Select top 10 locations\n",
    "top_10_locations = location_counts.head(10)\n",
    "\n",
    "colors = px.colors.qualitative.Set1[:10]  # Using a qualitative color palette for variety\n",
    "\n",
    "bar_plot(top_10_locations, 'Top 10 User Locations', 'user_location', 'count', 'user_location', colors, -45, (900, 600))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, most of the tweets in the dataset have origin in India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "df_tweets['source'].value_counts().nlargest(5).plot(kind='bar')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 5 Tweet Sources')\n",
    "\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.xlabel('Tweet Source')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are similar distributed among the biggest 3 platforms: Android, Web and Iphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_variation(df, x='date_only', y='count', hue = None, size=1, title=\"\", is_log=False):\n",
    "    f, ax = plt.subplots(1,1, figsize=(3*size,1*size))\n",
    "   \n",
    "    g = sns.lineplot(x=x, y=y, hue=hue, data=df)\n",
    "    #plt.xticks(rotation=90)\n",
    "    if hue:\n",
    "        plt.title(f'{y} grouped by {hue} | {title}')\n",
    "    else:\n",
    "        plt.title(f'{title}')\n",
    "    if(is_log):\n",
    "        ax.set(yscale=\"log\")\n",
    "    ax.grid(color='black', linestyle='dotted', linewidth=0.75)\n",
    "    plt.show()\n",
    "\n",
    "tweets_agg_df = df_tweets.groupby([\"date\"])[\"text\"].count().reset_index()\n",
    "tweets_agg_df.columns = [\"date_only\", \"count\"]\n",
    "\n",
    "plot_time_variation(tweets_agg_df, title=\"Number of Tweets per Day\", x='date_only', y='count', size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each country\n",
    "country_counts = df_tweets['user_location'].value_counts().reset_index()\n",
    "country_counts.columns = ['user_location', 'tweet_count']\n",
    "\n",
    "# Select top 10 countries with the most appearances in tweets\n",
    "top_10_countries = country_counts.nlargest(10, 'tweet_count')\n",
    "\n",
    "mean_sentiment_by_country = df_tweets.groupby('user_location')['vader_score'].mean().reset_index()\n",
    "mean_sentiment_by_country.columns = ['user_location', 'mean_sentiment']\n",
    "#select only countries in top 10\n",
    "mean_sentiment_by_country = mean_sentiment_by_country[mean_sentiment_by_country['user_location'].isin(top_10_countries['user_location'])]\n",
    "colors = px.colors.qualitative.Set1[:10]\n",
    "bar_plot(mean_sentiment_by_country, 'Mean Sentiment of Top 10 Countries with Most Appearances in Tweets', 'user_location', 'mean_sentiment', 'user_location', colors, -45, (900,600), .5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analisys reveals tweets from most proeminent geographic locations in the dataset had a rather positive sentiment towards COVID (most certainly not to COVID itself, but optimistic news), while in Toronto and in the rest of the world it was the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_sentiment(df):\n",
    "    return df['vader_score'].mean()\n",
    "\n",
    "# Calculate the mean sentiment score for each vaccine\n",
    "mean_sentiments = {\n",
    "    'Pfizer': calculate_mean_sentiment(df_tweets_pz),\n",
    "    'Sinopharm': calculate_mean_sentiment(df_tweets_sinopharm),\n",
    "    'Sinovac': calculate_mean_sentiment(df_tweets_sinovac),\n",
    "    'Moderna': calculate_mean_sentiment(df_tweets_moderna),\n",
    "    'AstraZeneca': calculate_mean_sentiment(df_tweets_astrazeneca),\n",
    "    'Covaxin': calculate_mean_sentiment(df_tweets_covaxin),\n",
    "    'Sputnik V': calculate_mean_sentiment(df_tweets_sputnik)\n",
    "}\n",
    "\n",
    "# Convert mean sentiments to a DataFrame\n",
    "mean_sentiments_df = pd.DataFrame(list(mean_sentiments.items()), columns=['Vaccine', 'Mean Sentiment']).sort_values('Mean Sentiment', ascending=False)\n",
    "colors = px.colors.qualitative.Set1[:7]\n",
    "\n",
    "# Plot the mean sentiment scores for each vaccine\n",
    "bar_plot(mean_sentiments_df, 'Mean Sentiment Scores for Each Vaccine', 'Vaccine', 'Mean Sentiment', 'Vaccine', colors, -45, (900, 600), .15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, all vacines had rather positive views from the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patterns in a Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trend\n",
    "\n",
    "A trend in a time series refers to the movement or direction of data over a period o time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seasonality\n",
    "\n",
    "A seasonality refers to the presence of predictable patterns that happen over a specific period due to seasonal factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand if the data has either a trend or a seasonality we will take a look at the number of tweets over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_variation(tweets_agg_df, title=\"Number of Tweets per Day\", x='date_only', y='count', size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the COVID-19 vaccine tweets dataset, it appears that there are no significant trends or seasonality.\n",
    "The time series decomposition reveals a relatively flat trend component, indicating no long-term increase or decrease in tweet volumes.\n",
    "\n",
    "Additionally, the seasonal component does not show any consistent, repeating patterns over time.\n",
    "\n",
    "\n",
    "Therefore, the data seems to be mostly influenced by random fluctuations rather than systematic trends or seasonal effects, which makes sense since the number of tweets are driven by random, unpredictable events such as news reports, vaccine rollouts, public health announcements, and social media campaigns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition of a Time Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# Multiplicative Decomposition \n",
    "result_mul = seasonal_decompose(tweets_agg_df['count'], model='multiplicative', extrapolate_trend='freq', period=150)\n",
    "\n",
    "# Additive Decomposition\n",
    "result_add = seasonal_decompose(tweets_agg_df['count'], model='additive', extrapolate_trend='freq', period=150)\n",
    "\n",
    "# Plot\n",
    "plt.rcParams.update({'figure.figsize': (16,12)})\n",
    "result_mul.plot().suptitle('Multiplicative Decomposition', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "result_add.plot().suptitle('Additive Decomposition', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationary or Non-Stationarity Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to understand whether our time series is stationary or not. A stationary time series is one whose statistical properties such as mean, variance, and autocorrelation are all constant over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented Dickey Fuller test (ADF Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "result = adfuller(tweets_agg_df['count'])\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude  based on the results that the time series is not stationary.\n",
    "\n",
    "In this test, we assume that the null hypothesis is the time series possesses a unit root and is non-stationary.\n",
    "\n",
    "* Since the ADF statistic (-1.887130) is not less than the critical values at the 1%, 5%, or even 10% significance levels, you cannot reject the null hypothesis that the time series has a unit root.\n",
    "\n",
    "* Additionally, the p-value (0.338140) is significantly higher than 0.05, which further indicates that you cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kwiatkowski-Phillips-Schmidt-Shin – KPSS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kpss(tweets_agg_df['count'])\n",
    "\n",
    "print('\\nKPSS Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[3].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KPSS Test also supports the conclusion that the time series is not stationary.\n",
    "\n",
    "* The KPSS statistic (0.621666) is greater than the critical values at the 10%, 5%, and 2.5% significance levels.\n",
    "\n",
    "* The p-value is less than 0.05, which indicates that you can reject the null hypothesis that the time series is stationary.\n",
    "\n",
    "A major difference between KPSS and ADF tests is the capability of the KPSS test to check for stationarity in the ‘presence of a deterministic trend’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Draw Plot\n",
    "plt.rcParams.update({'figure.figsize':(10,6), 'figure.dpi':120})\n",
    "autocorrelation_plot(tweets_agg_df['count'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation and Partial Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autocorrelation is simply the correlation of a series with its own lags. If a series is significantly autocorrelated, that means, the previous values of the series (lags) may be helpful in predicting the current value.\n",
    "\n",
    "* Partial Autocorrelation also conveys similar information but it conveys the pure correlation of a series and its lag, excluding the correlation contributions from the intermediate lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Draw Plot\n",
    "fig, axes = plt.subplots(1,2,figsize=(16,3), dpi= 100)\n",
    "plot_acf(tweets_agg_df['count'].tolist(), lags=50, ax=axes[0])\n",
    "plot_pacf(tweets_agg_df['count'].tolist(), lags=50, ax=axes[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The presence of significant spikes in both plots suggests that there is autocorrelation at lags 1 and 2.\n",
    "\n",
    "Let's consider an ARIMA model with p (number of lag observations included in the model) equal to 2 and q (size of the moving average window) equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of tweets per day of the week\n",
    "df_dayofweek = df_tweets.groupby('dayofweek').size().reset_index(name='count')\n",
    "df_dayofweek['dayofweek'] = df_dayofweek['dayofweek'].map({0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'})\n",
    "\n",
    "df_dayofweek['dayofweek'] = pd.Categorical(df_dayofweek['dayofweek'], categories=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], ordered=True)\n",
    "\n",
    "colors = px.colors.qualitative.Set1\n",
    "bar_plot(df_dayofweek, 'Number of Tweets per Day of the Week', 'dayofweek', 'count', 'dayofweek', colors, 0, (900, 600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets regarding COVID were more proeminent during the week days than during the weekend. It may be related to the scheduled release of weekly reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of tweets per hour\n",
    "df_hour = df_tweets.groupby('hour').size().reset_index(name='count')\n",
    "df_hour = df_hour.sort_values('count', ascending=False)\n",
    "\n",
    "px.colors.qualitative.Vivid\n",
    "colors = px.colors.qualitative.Set2[:10]\n",
    "scale = \"aggrnyl\"\n",
    "bar_plot(df_hour, 'Number of Tweets per Hour', 'hour', 'count', 'count', colors, 0, (900, 600),0,scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, most tweets happened during day hours, with a peak from 13h to 15h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tweet_counts_hourly = df_tweets.groupby(['year', 'month', 'day', 'hour']).size().reset_index(name='tweet_count')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "tweet_counts_hourly['date'] = pd.to_datetime(tweet_counts_hourly[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "# Drop redundant columns\n",
    "tweet_counts_hourly.drop(columns=['year', 'month', 'day', 'hour'], inplace=True)\n",
    "\n",
    "# Set 'date' column as index\n",
    "tweet_counts_hourly.set_index('date', inplace=True)\n",
    "tweet_counts_hourly = tweet_counts_hourly.asfreq('H')\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tweet_counts_hourly.index, tweet_counts_hourly['tweet_count'])\n",
    "plt.title('Hourly Tweet Counts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "\n",
    "sample = tweet_counts_hourly[tweet_counts_hourly.index.month == 8]\n",
    "sample = sample[sample.index.year == 2021]\n",
    "# days from 1 to 4\n",
    "sample = sample[sample.index.day <= 20]\n",
    "\n",
    "# Split the data into train and test\n",
    "train_size = int(len(sample) * 0.8)\n",
    "train, test = sample[0:train_size], sample[train_size:len(sample)]\n",
    "\n",
    "# Fit the ARIMA model on the training dataset\n",
    "model_train = ARIMA(train, order=(2,1,2))\n",
    "model_train_fit = model_train.fit()\n",
    "\n",
    "# Forecast on the test dataset\n",
    "test_forecast = model_train_fit.get_forecast(steps=len(test))\n",
    "test_forecast_series = pd.Series(test_forecast.predicted_mean, index=test.index)\n",
    "\n",
    "test = test.dropna()\n",
    "test_forecast_series = test_forecast_series.dropna()\n",
    "\n",
    "test_forecast_series = test_forecast_series.loc[test.index]\n",
    "\n",
    "mse = mean_squared_error(test['tweet_count'], test_forecast_series)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(test['tweet_count'], test_forecast_series)\n",
    "rmse = mse**0.5\n",
    "\n",
    "\n",
    "# Create a plot to compare the forecast with the actual test data\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train, label='Training Data', color='blue')\n",
    "\n",
    "plt.plot(test, label='Actual Test Data', color='green')\n",
    "plt.plot(test_forecast_series, label='Forecast', color='red')\n",
    "\n",
    "plt.title('ARIMA Model Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "\n",
    "# Format the x-axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)  # Rotate date labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of date labels\n",
    "plt.show()\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High RMSE shows low seasonality in the dataset and how hard it would be to predict tweet count regarding a random event like the COVID epidemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate tweet counts by day\n",
    "tweet_counts_daily = df_tweets.groupby(['year', 'month', 'day']).size().reset_index(name='tweet_count')\n",
    "\n",
    "# Convert date columns to datetime format\n",
    "tweet_counts_daily['date'] = pd.to_datetime(tweet_counts_daily[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# Drop redundant columns\n",
    "tweet_counts_daily.drop(columns=['year', 'month', 'day'], inplace=True)\n",
    "\n",
    "# Set 'date' column as index\n",
    "\n",
    "tweet_counts_daily.set_index('date', inplace=True)\n",
    "tweet_counts_daily = tweet_counts_hourly.asfreq('D')\n",
    "\n",
    "# Plot the time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tweet_counts_daily.index, tweet_counts_daily['tweet_count'])\n",
    "plt.title('Daily Tweet Counts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High RMSE shows low seasonality in the dataset and how hard it would be to predict tweet count regarding a random event like the COVID epidemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = tweet_counts_daily[tweet_counts_daily.index.month >= 2]\n",
    "data_sample = data_sample[data_sample.index.month <= 4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing a model\n",
    "Exponential smoothings methods are appropriate for non-stationary data (ie data with a trend and seasonal data).\n",
    "\n",
    "ARIMA models should be used on stationary data only. One should therefore remove the trend of the data (via deflating or logging), and then look at the differenced series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing methods\n",
    "Smoothing methods work as weighted averages. Forecasts are weighted averages of past observations. The weights can be uniform (this is a moving average), or following an exponential decay — this means giving more weight to recent observations and less weight to old observations. More advanced methods include other parts in the forecast, like seasonal components and trend components.\n",
    "\n",
    "We will use the component form for our mathematical equations. y will denote our time series, p our forecast, l the level, s the seasonal component and b the trend component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Exponential Smoothing\n",
    "\n",
    "When to use?\n",
    "* Few data points, Irregular data, No seasonality or trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing\n",
    "\n",
    "\n",
    "# Set the value of Alpha and define m (Time Period)\n",
    "m = 12\n",
    "alpha = 1/(2*m)\n",
    "\n",
    "data_sample['alpha.2'] = SimpleExpSmoothing(data_sample['tweet_count']).fit(smoothing_level=.2,optimized=False).fittedvalues\n",
    "data_sample['alpha.5'] = SimpleExpSmoothing(data_sample['tweet_count']).fit(smoothing_level=.5,optimized=False).fittedvalues\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(data_sample['tweet_count'], label='Actual')\n",
    "plt.plot(data_sample['alpha.2'], label='alpha= 0.2')\n",
    "plt.plot(data_sample['alpha.5'], label='alpha= 0.5')\n",
    "plt.title('Simple Exponential Smoothing')\n",
    "\n",
    "# Format the x-axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of date labels\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holt's Exponential Smoothing\n",
    "\n",
    "When to use?\n",
    "* Trend in data, No seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample['holt_.05'] = Holt(data_sample['tweet_count']).fit(smoothing_level=.3, smoothing_slope=.05,optimized=False).fittedvalues\n",
    "data_sample['holt_.2'] = Holt(data_sample['tweet_count']).fit(smoothing_level=.3, smoothing_slope=.2,optimized=False).fittedvalues\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(data_sample['tweet_count'], label='Actual')\n",
    "plt.plot(data_sample['holt_.05'], label='alpha= 0.3 & beta= 0.05')\n",
    "plt.plot(data_sample['holt_.2'], label='alpha= 0.3 & beta= 0.2')\n",
    "plt.title('Holt\\'s Exponential Smoothing')\n",
    "\n",
    "# Format the x-axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of date labels\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holt’s Damped Trend\n",
    "\n",
    "The problem with Holt’s Linear trend method is that the trend is constant in the future, increasing or decreasing indefinitely. For long forecast horizons, this can be problematic. The damped trend method is therefore a method which add a dampening parameter so that the trend converges to a constant value in the future (it flattens the trend). The parameter 𝑏 is replaced by 𝜙𝑏\n",
    "\n",
    "When to use?\n",
    "* Data has a trend. Use the multiplicative version, unless the data has been logged before. In this case, use the additive version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample['HWES3_ADD'] = ExponentialSmoothing(data_sample['tweet_count'],trend='add',seasonal='add',seasonal_periods=12).fit().fittedvalues\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(data_sample['tweet_count'], label='Actual')\n",
    "\n",
    "plt.plot(data_sample['HWES3_ADD'], label='Triple Exponential Smoothing', color='green')\n",
    "plt.title('Triple Exponential Smoothing')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "\n",
    "# Format the x-axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of date labels\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite not achieving great success using exponential smoothing for forecasting, triple exponential smoothing proved to be substatially more successful in comparision to other strategies, being able to predict the overall tendency of the tweet count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_size = int(len(data_sample) * 0.7)\n",
    "train, test = data_sample[0:train_size], data_sample[train_size:len(data_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fit the Holt-Winters model on the training dataset\n",
    "fitted_model = ExponentialSmoothing(train['tweet_count'],trend='add',seasonal='add',seasonal_periods=12).fit()\n",
    "\n",
    "# Forecast on the test dataset\n",
    "test_predictions = fitted_model.forecast(len(test))\n",
    "\n",
    "# Calculate the mean squared error\n",
    "\n",
    "\n",
    "# Create a plot to compare the forecast with the actual test data\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train['tweet_count'], label='Training Data', color='blue')\n",
    "plt.plot(test['tweet_count'], label='Actual Test Data', color='green')\n",
    "plt.plot(test_predictions, label='Forecast', color='red')\n",
    "plt.title('Holt-Winters Model Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "# Format the x-axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of date labels\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA\n",
    "\n",
    "ARIMA models (which include ARMA, AR and MA models) are a general class of models to forecast stationary time series.\n",
    "\n",
    "An ARIMA model is often noted ARIMA(p, d, q) where p represents the order of the AR part, d the order of differencing (“I” part), and q the order of the MA term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Choosing the differencing order\n",
    "\n",
    "The first step of fitting an ARIMA model is to determine the differencing order to stationarize the series. To do that, we look at the ACF and PACF plots, and keep in mind these two rules:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "sample = tweet_counts_hourly[tweet_counts_hourly.index.month == 8]\n",
    "sample = sample[sample.index.year == 2021]\n",
    "# days from 1 to 4\n",
    "sample = sample[sample.index.day <= 20]\n",
    "\n",
    "# Split the data into train and test\n",
    "train_size = int(len(sample) * 0.8)\n",
    "train, test = sample[0:train_size], sample[train_size:len(sample)]\n",
    "\n",
    "# Fit the ARIMA model on the training dataset\n",
    "model_train = ARIMA(train, order=(2,1,2))\n",
    "model_train_fit = model_train.fit()\n",
    "\n",
    "# Forecast on the test dataset\n",
    "test_forecast = model_train_fit.get_forecast(steps=len(test))\n",
    "test_forecast_series = pd.Series(test_forecast.predicted_mean, index=test.index)\n",
    "\n",
    "test = test.dropna()\n",
    "test_forecast_series = test_forecast_series.dropna()\n",
    "\n",
    "test_forecast_series = test_forecast_series.loc[test.index]\n",
    "\n",
    "mse = mean_squared_error(test['tweet_count'], test_forecast_series)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(test['tweet_count'], test_forecast_series)\n",
    "rmse = mse**0.5\n",
    "\n",
    "\n",
    "# Create a plot to compare the forecast with the actual test data\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(train, label='Training Data', color='blue')\n",
    "\n",
    "plt.plot(test, label='Actual Test Data', color='green')\n",
    "plt.plot(test_forecast_series, label='Forecast', color='red')\n",
    "\n",
    "plt.title('ARIMA Model Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Tweet Count')\n",
    "\n",
    "# Format the x-axis dates\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)  # Rotate date labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of date labels\n",
    "plt.show()\n",
    "\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
