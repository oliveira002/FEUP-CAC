{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Covid-19 Vaccines - Sentiment Analysis & Time Series**\n",
    "Notebook for the second project for the Machine Learning Complements course (CAC).\n",
    "\n",
    "## **Introduction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The following libraries will be used in this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils as ut\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "nltk.download('vader_lexicon')\n",
    "warnings.simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Observations\n",
    "\n",
    "The dataset contains a single file: `tweets.csv`.\n",
    "\n",
    "In this section we will take a look at the first few rows of each file to get a better understanding of the data, and do some initial data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.initial_obs(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "We can see that many attributes are not really relevant for the kind of work we will be doing. Therefore, we'll just selec tthe most relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = df_tweets[['id','user_location','date','text','hashtags']]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Spaces within the text\n",
    "When removing spaces within the text, ensure seamless integration of words for enhanced readability and processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.trim_text)\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions Mapping\n",
    "Contractions mapping simplifies language by expanding contractions like \"can't\" to \"cannot\" for consistent analysis and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(contractions.fix)\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning HTML\n",
    "Cleaning HTML tags from text data streamlines content for NLP tasks, preventing interference from markup elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "df_tweets['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emojis & Emotion Handling\n",
    "Emojis and emotion handling enrich text analysis by capturing nuances of sentiment and expression for deeper understanding. We thought about removing them initially, however their presence may be crucial to identify sentiments within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251\\U0001F004\\U0001F0CF\\U0001F170-\\U0001F251\\U0001F600-\\U0001F64F\\U0001F680-\\U0001F6FF]+', flags=re.UNICODE)\n",
    "\n",
    "# Find examples in df_tweets['text'] that have emojis\n",
    "emojis_examples = df_tweets[df_tweets['text'].str.contains(pattern, na=False)][0:5]\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    print(df_tweets.loc[index, 'text'])\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].apply(ut.convert_emojis_to_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    emoji_text = df_tweets.loc[index, 'text']\n",
    "    print(emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Twitter Handles (@) & Hashtags\n",
    "Handling Twitter handles (@) and hashtags facilitates contextual analysis and topic extraction in social media text. We removed the twitter handle, as they mostly are used to identify persons therefore they are not very important in this matter. On the other hand, hashtags may indicate sentiments or other important informations like topics. e.g #sad, #happy or #astrozeneca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.remove_twitter_handles_hashtags)\n",
    "\n",
    "for index in emojis_examples.index:\n",
    "    emoji_text = df_tweets.loc[index, 'text']\n",
    "    print(emoji_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lower-case\n",
    "Converting all the characters to lower case so that words in different forms can be interpreted as the same. The problem with this is that in social media people may use upper-case to express sentiments, e.g SAD, HAPPY.\n",
    "\n",
    "Here we also remove special characters, keeping only characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['text'] = df_tweets['text'].apply(ut.remove_special_characters)\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n",
    "\n",
    "df_tweets['text'] = df_tweets['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization breaks down text into individual units, such as words or phrases, enabling granular analysis and feature extraction. We also remove stopwords, meaning words that often appear within the text and don't add any meaning to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tokenized_text'] = df_tweets['text'].apply(lambda x: word_tokenize(x))\n",
    "df_tweets['tokenized_text'] = df_tweets['tokenized_text'].apply(ut.remove_stopwords)\n",
    "df_tweets['token_text'] = df_tweets['tokenized_text'].apply(lambda text: \" \".join(text))\n",
    "\n",
    "\n",
    "df_tweets['tokenized_text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming typically chops off prefixes and/or suffixes of words to derive the root form. It's a simpler and faster process compared to lemmatization. However, stemming doesn't always result in valid words. For instance, \"running\" might be stemmed to \"runn,\" which isn't a valid word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "df_tweets['stemmed_text'] = df_tweets['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "df_tweets['stemmed_text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "Lemmatization, on the other hand, involves resolving words to their dictionary form, known as the lemma. It uses lexical knowledge bases to ensure that the root form returned is a valid word. For example, \"am,\" \"are,\" and \"is\" would all be lemmatized to \"be.\" Lemmatization is generally more accurate than stemming but can be slower due to its linguistic complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "df_tweets['lemmatized_text'] = df_tweets['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "df_tweets['lemmatized_text'].head(5)\n",
    "df_tweets['clean_text'] = df_tweets['lemmatized_text'].apply(lambda text: \" \".join(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    vader_scores = sid.polarity_scores(text)['compound']\n",
    "    if vader_scores >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif vader_scores <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    return sentiment, vader_scores\n",
    "\n",
    "df_tweets['sentiment'], df_tweets['vader_score'] = zip(*df_tweets['text'].apply(analyze_sentiment))\n",
    "#df_tweets['sentiment'] = df_tweets['sentiment'].replace({'Positive': 1, 'Neutral': 0, 'Negative': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_sentiments(df_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordClouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = df_tweets[df_tweets['sentiment'] == \"Positive\"]\n",
    "negative_tweets = df_tweets[df_tweets['sentiment'] == \"Negative\"]\n",
    "neutral_tweets = df_tweets[df_tweets['sentiment'] == \"Neutral\"]\n",
    "\n",
    "ut.generate_word_cloud(positive_tweets['token_text'], 'Positive Sentiment Word Cloud')\n",
    "positive_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(positive_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neutral Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.generate_word_cloud(neutral_tweets['token_text'], 'Neutral Sentiment Word Cloud')\n",
    "neutral_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(neutral_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sentiment - WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.generate_word_cloud(negative_tweets['token_text'], 'Negative Sentiment Word Cloud')\n",
    "negative_tweets['clean_text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.common_words(negative_tweets, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Analysis by sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uni-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tri-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_n_grams(df_tweets, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Average Word Amount by Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_avg_word_length_distribution_multi(positive_tweets, neutral_tweets, negative_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Typical ML Approach\n",
    "As we can see our dataset is not labelled, therefore we can't separate it into train/test and just train a model. What we will do is train a model using a labelled tweet dataset (the theme of both dataset would be similar, so we can use it for training) and then test on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training_df = pd.read_csv('train_tweets.csv')\n",
    "training_df = training_df[training_df['new_sentiment'].notna()]\n",
    "training_df = training_df[['old_text','new_sentiment']]\n",
    "\n",
    "\n",
    "training_df = ut.pre_process_pipeline(training_df,'old_text')\n",
    "training_df.rename(columns={'new_sentiment': 'sentiment'}, inplace=True)\n",
    "training_df['sentiment'] = training_df['sentiment'].replace({'positive': 1, 'neutral': 0, 'negative': -1})\n",
    "\n",
    "ut.initial_obs(training_df)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Training-Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_features, test_features = ut.generate_features(training_df,df_tweets,'tfidf')\n",
    "\n",
    "#TF-IDF Results:\n",
    "pred_nb_tfidf, pred_svm_tfidf = ut.predict_labels(train_features,training_df['sentiment'].values,test_features)\n",
    "\n",
    "df_tweets['predicted_tfidf_NB'] = pred_nb_tfidf\n",
    "df_tweets['predicted_tfidf_SVM'] = pred_svm_tfidf\n",
    "\n",
    "\n",
    "df_eval = df_tweets.copy()\n",
    "df_eval = df_eval[['clean_text','text','sentiment','predicted_tfidf_NB','predicted_tfidf_SVM']]\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sentiment_counts = df_eval['predicted_tfidf_NB'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=\"viridis\")\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Spatial Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
